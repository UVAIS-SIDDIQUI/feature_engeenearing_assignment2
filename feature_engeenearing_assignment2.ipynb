{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd9b190-c926-4a7e-87a7-9f613ca38eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "# application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce3336-47f1-4623-b49f-b76dd97795f9",
   "metadata": {},
   "source": [
    "ANS = Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale features to a fixed range, typically [0, 1] or [-1, 1]. This technique ensures that all features contribute equally to the model, especially when the features are measured on different scales.\n",
    "\n",
    "The Min-Max scaling formula for a feature \n",
    "ùë•\n",
    "x is: ùë•‚Ä≤=ùë•‚àíùë•min/ùë•max‚àíùë•min\n",
    "\n",
    "where:\n",
    "\n",
    "x is the original value.\n",
    "\n",
    "ùë• min is the minimum value of the feature.\n",
    "\n",
    "ùë•max is the maximum value of the feature.\n",
    "\n",
    "ùë•‚Ä≤ is the scaled value.\n",
    "\n",
    "\n",
    "Steps to Perform Min-Max Scaling\n",
    "\n",
    "Identify the minimum and maximum values of the feature.\n",
    "\n",
    "Subtract the minimum value from each data point.\n",
    "\n",
    "Divide the result by the range (maximum value - minimum value).\n",
    "\n",
    "Example\n",
    "\n",
    "Consider a dataset with a single feature:\n",
    "\n",
    "To apply Min-Max scaling to this feature:\n",
    "\n",
    "Find the minimum value (ùë•min and the maximum value (ùë•max):\n",
    "\n",
    "x min=20\n",
    "\n",
    "x max=100\n",
    "\n",
    "Apply the Min-Max scaling formula to each data point:\n",
    "\n",
    "x‚Ä≤=x‚àí20/100‚àí20\n",
    "\n",
    "The scaled feature values now range from 0 to 1.\n",
    "\n",
    "Applications\n",
    "\n",
    "Min-Max scaling is commonly used in machine learning algorithms, particularly those involving distance measurements (e.g., K-nearest neighbors, clustering algorithms) and gradient-based optimization methods (e.g., neural networks). By scaling the features, we ensure that the algorithm performs optimally and that the features contribute equally to the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53b46a1-ae63-4111-bf28-f37d9b805cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "# Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ceaae-fead-4884-b985-3650cd3a8aa5",
   "metadata": {},
   "source": [
    "ANS = The Unit Vector technique, also known as vector normalization or the normalization method, scales each data point (or vector) in the dataset to have a unit norm (usually 1). This means the length (or magnitude) of each vector becomes 1, but the direction of the vectors remains unchanged. This technique is particularly useful in scenarios where the direction of the data is more important than its magnitude, such as in text analysis and cosine similarity calculations.\n",
    "\n",
    "Formula\n",
    "The formula for unit vector scaling (normalization) for a vector \n",
    "ùë•= [ùë•1,ùë•2,‚Ä¶,ùë•ùëõ] is: \n",
    "\n",
    "x‚Ä≤= x/‚à•x‚à• \n",
    "\n",
    "where \n",
    "‚à•\n",
    "ùë•\n",
    "‚à•\n",
    "‚à•x‚à• is the norm (magnitude) of the vector, commonly calculated as the Euclidean norm:\n",
    "\n",
    "‚à•ùë•‚à•=ùë•12+x2^2+‚Ä¶xùëõ^2\n",
    "\n",
    "Steps to Perform Unit Vector Scaling\n",
    "\n",
    "Compute the norm (magnitude) of the vector.\n",
    "\n",
    "Divide each component of the vector by its norm.\n",
    "\n",
    "Example\n",
    "\n",
    "Consider a dataset with two features (2D vectors):\n",
    "\n",
    "To apply Unit Vector scaling to each data point:\n",
    "\n",
    "Compute the norm for each vector:\n",
    "\n",
    "Comparison with Min-Max Scaling\n",
    "\n",
    "Min-Max Scaling: Rescales the features to a specific range (e.g., [0, 1]), transforming each feature independently.\n",
    "Unit Vector Scaling: Rescales the entire vector to have a unit norm, preserving the direction but changing the magnitude to 1.\n",
    "Applications\n",
    "Unit Vector scaling is often used in machine learning algorithms where the direction of the data points is more important than their magnitude, such as:\n",
    "\n",
    "Cosine Similarity: \n",
    "Often used in text analysis and recommendation systems.\n",
    "Neural Networks: To ensure consistent input scale, helping with faster convergence.\n",
    "Clustering Algorithms: Such as K-means, where the similarity between data points (e.g., cosine similarity) is more relevant than their absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2c2234-61dd-400e-8193-a03c1b044be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "# example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde9a51-d651-4a7d-9f5c-0299ffa524ee",
   "metadata": {},
   "source": [
    "ANS = Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a dataset with many variables into a smaller set of variables that still contains most of the information in the original dataset. PCA achieves this by identifying the principal components, which are new, uncorrelated variables that successively maximize variance.\n",
    "\n",
    "Steps in PCA\n",
    "Standardize the Data: Scale the data so that each feature has a mean of zero and a standard deviation of one.\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix to understand how variables in the dataset relate to one another.\n",
    "Calculate Eigenvalues and Eigenvectors: Determine the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors determine the directions of the new feature space, while eigenvalues determine their magnitude.\n",
    "Sort Eigenvalues and Eigenvectors: Sort the eigenvalues and their corresponding eigenvectors in decreasing order of eigenvalues. The eigenvectors corresponding to the largest eigenvalues form the principal components.\n",
    "Transform the Data: Project the original data onto the new feature space defined by the top eigenvectors.\n",
    "Example\n",
    "Let's say we have a dataset with two features, ùëã1 and  X1 and we want to reduce it to one principal component.\n",
    "\n",
    "Step-by-Step PCA\n",
    "Standardize the Data\n",
    "\n",
    "Mean of x1 is 3.5, mean of X2 is 4.75.Standardize each feature.\n",
    "\n",
    "Compute Covariance Matrix\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors\n",
    "\n",
    "Suppose the covariance matrix yields eigenvalues 1.8 and 0.2, with corresponding eigenvectors \n",
    "[0.707,0.707] and[‚àí0.707,0.707].Sort Eigenvalues and EigenvectorsLargest eigenvalu is 1.8 with eigenvector [0.707,0.707].Transform the Data\n",
    "\n",
    "Project the original data onto the new principal component (the eigenvector with the largest eigenvalue).\n",
    "\n",
    "\n",
    "The transformed data represents the original data in a single dimension, which is the principal component.\n",
    "\n",
    "Applications of PCA\n",
    "Data Visualization: Reducing data to 2 or 3 dimensions for visualization.\n",
    "Noise Reduction: Eliminating less significant components to reduce noise.\n",
    "Feature Extraction: Identifying significant features in the data.\n",
    "Data Compression: Reducing the storage space required for the data.\n",
    "By transforming the dataset into principal components, PCA helps simplify the dataset, making it easier to analyze and interpret while retaining most of the important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83d0aa3-28d2-4b1a-a3d6-057bb2d3aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "# Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcb0c0-7a1e-4233-80e8-fca1d43e22ed",
   "metadata": {},
   "source": [
    "ANS = PCA (Principal Component Analysis) and feature extraction are closely related in that PCA is a powerful technique used for feature extraction. Feature extraction involves transforming the data into a set of features that are more useful for analysis while retaining essential information. PCA achieves this by identifying the principal components, which are new features that capture the most variance in the data.\n",
    "\n",
    "Relationship between PCA and Feature Extraction\n",
    "Dimensionality Reduction: PCA reduces the number of features while retaining the most significant information. This reduced set of features (principal components) represents the original data with minimal loss of information.\n",
    "New Feature Space: PCA transforms the original features into a new set of features (principal components) that are orthogonal and uncorrelated. These new features often reveal underlying patterns in the data.\n",
    "Variance Maximization: The principal components are ordered by the amount of variance they capture from the original data. The first few components usually capture most of the variability, making them ideal for feature extraction.\n",
    "How PCA is Used for Feature Extraction\n",
    "Standardize the Data: Ensure each feature has a mean of zero and a standard deviation of one.\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "Determine Eigenvalues and Eigenvectors: Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Select Principal Components: Choose the top \n",
    "ùëò\n",
    "k eigenvectors (principal components) that correspond to the largest eigenvalues.\n",
    "Transform the Data: Project the original data onto the selected principal components to obtain the new feature set.\n",
    "Example\n",
    "Consider a dataset with three features: \n",
    "\n",
    "ùëã1,ùëã2 and X3\n",
    "\n",
    "Step-by-Step PCA for Feature Extraction\n",
    "Standardize the Data\n",
    "\n",
    "Standardize each feature to have zero mean and unit variance.\n",
    "Compute Covariance Matrix\n",
    "\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "Determine Eigenvalues and Eigenvectors\n",
    "\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Select Principal Components\n",
    "\n",
    "Suppose the eigenvalues are 2.5, 1.2, and 0.3 with corresponding eigenvectors [ùë£1,ùë£2,ùë£3]. Select the top two eigenvectors (ùë£1 and ùë£2)\n",
    "Transform the Data\n",
    "\n",
    "Project the original data onto the new feature space defined by the selected eigenvectors.\n",
    "Transformed Data (Principal Components)\n",
    "\n",
    "Here, PC1 and PC2 are the new features (principal components) extracted from the original dataset. These new features capture most of the variability in the original data.\n",
    "\n",
    "Benefits of Using PCA for Feature Extraction\n",
    "Simplifies Models: Reduces the complexity of models by decreasing the number of features.\n",
    "Improves Performance: Enhances the performance of machine learning algorithms by eliminating multicollinearity and reducing overfitting.\n",
    "Enhances Interpretability: Makes the data easier to visualize and interpret by reducing it to a lower-dimensional space.\n",
    "\n",
    "By using PCA for feature extraction, we can create a simplified and efficient representation of the original data, facilitating better analysis and decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a6d614-a5fc-44be-a133-ba2d0b2583c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "# contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "# preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7bb99-25a6-441d-99c6-56e46deca5a3",
   "metadata": {},
   "source": [
    "ANS = Min-Max scaling is a normalization technique used to preprocess data, bringing all feature values into a common scale without distorting differences in the ranges of values. This is particularly useful in recommendation systems where different features may have different ranges and units. By using Min-Max scaling, we ensure that all features contribute equally to the model.\n",
    "\n",
    "Steps for Min-Max Scaling\n",
    "Identify Feature Ranges: Determine the minimum and maximum values for each feature in the dataset.\n",
    "Apply the Scaling Formula: Transform the data using the Min-Max scaling formula:\n",
    "\n",
    "X scaled = X‚àíXmin/Xmax - X min\n",
    "\n",
    "Here, X is the original feature value, ùëãmin is the minimum value of the feature, and ùëãmax is the maximum value of the feature. The result, ùëãscaled, is the scaled value.\n",
    "Example: Preprocessing the Food Delivery Dataset\n",
    "\n",
    "Let's say our dataset has the following features: \n",
    "\n",
    "price, rating, and delivery time.\n",
    "\n",
    "Step-by-Step Min-Max Scaling\n",
    "Identify Minimum and Maximum Values for Each Feature\n",
    "\n",
    "Price: min = 10, max = 25\n",
    "\n",
    "Rating: min = 3.5, max = 5.0\n",
    "\n",
    "Delivery Time: min = 20, max = 45\n",
    "\n",
    "Apply the Scaling Formula\n",
    "\n",
    "For Price:\n",
    "\n",
    "Benefits of Min-Max Scaling in the Recommendation System\n",
    "Uniform Contribution: Ensures that each feature contributes equally to the similarity measures used in the recommendation algorithm.\n",
    "Improved Convergence: Helps in faster convergence of gradient descent-based optimization algorithms.\n",
    "Enhanced Performance: Improves the performance of distance-based algorithms (e.g., k-nearest neighbors) by making feature ranges comparable.\n",
    "By preprocessing the data using Min-Max scaling, we create a level playing field for all features, enabling more accurate and efficient recommendations in the food delivery service system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e87723-c8f5-4bc2-8d53-f4ecffb24430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "# features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "# dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe1d41-5ce7-4e5c-a3af-b345052d166e",
   "metadata": {},
   "source": [
    "ANS =  Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices can help in simplifying the model, reducing overfitting, and improving computational efficiency. Here‚Äôs a detailed step-by-step approach to applying PCA in this context:\n",
    "\n",
    "Step-by-Step Process for Using PCA\n",
    "Data Preparation\n",
    "\n",
    "Collect Data: Gather the dataset, which includes features such as company financial data (e.g., earnings, revenue, P/E ratio) and market trends (e.g., stock index values, trading volume).\n",
    "Handle Missing Values: Impute or remove any missing values in the dataset.\n",
    "Standardize Features: Standardize the features so that they have a mean of zero and a standard deviation of one, as PCA is sensitive to the scale of the data.\n",
    "Compute the Covariance Matrix\n",
    "\n",
    "Calculate the covariance matrix of the standardized data to understand the relationships between different features.\n",
    "Calculate Eigenvalues and Eigenvectors\n",
    "\n",
    "Determine the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the amount of variance captured by each principal component, and the eigenvectors indicate the direction of these components.\n",
    "Select Principal Components\n",
    "\n",
    "Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues. These top k eigenvectors form the principal components.\n",
    "Transform the DataProject the original standardized data onto the new feature space defined by the selected principal components to obtain the reduced dataset.\n",
    "Build the Predictive Model\n",
    "\n",
    "Use the transformed dataset with reduced dimensions as input to your predictive model (e.g., linear regression, decision trees, neural networks).\n",
    "Example\n",
    "\n",
    "Original Data\n",
    "Assume the dataset has features such as:\n",
    "\n",
    "Company financial data: earnings, revenue, P/E ratio, etc.\n",
    "Market trends: stock index values, trading volume, etc.\n",
    "Step-by-Step Application of PCA\n",
    "Standardize the Data\n",
    "\n",
    "Standardize each feature to have zero mean and unit variance.\n",
    "Compute Covariance Matrix\n",
    "\n",
    "Compute the covariance matrix of the standardized data.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors\n",
    "\n",
    "Suppose the covariance matrix yields eigenvalues \n",
    "ùúÜ1,ùúÜ2,‚Ä¶,ùúÜùëõ and corresponding eigenvectors ùë£1,ùë£2,‚Ä¶ùë£ùëõ Select Principal Components\n",
    "Sort the eigenvalues and select the top k eigenvectors (e.g.,ùë£1,ùë£2,ùë£3).\n",
    "Transform the DataProject the original data onto the new feature space defined by the top \n",
    "k eigenvectors:\n",
    "Transformed¬†Data=Original¬†Data√ó[ùë£1,ùë£2,ùë£3]\n",
    "\n",
    "Building the Predictive Model\n",
    "Train-Test Split: Split the transformed dataset into training and testing sets.\n",
    "Model Training: Train your predictive model (e.g., linear regression, random forest) using the training set.\n",
    "Model Evaluation: Evaluate the model‚Äôs performance using the testing set.\n",
    "Benefits of Using PCA\n",
    "Dimensionality Reduction: Reduces the number of features, simplifying the model.\n",
    "Improved Performance: Helps in reducing overfitting by removing noise and redundant features.\n",
    "Computational Efficiency: Decreases computational requirements, making the model faster to train and predict.\n",
    "Enhanced Interpretability: Makes it easier to visualize and interpret the impact of the principal components.\n",
    "By applying PCA, you can focus on the most important features that capture the maximum variance in the data, leading to a more efficient and potentially more accurate stock price prediction model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c14169e-1de1-4582-b509-7464c002ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "# values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee26fd0d-16ae-4df8-9794-11b0feadd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e962068-de80-420b-9465-65c6e2a2ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'values': [1, 5, 10, 15, 20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3413c8-25d2-4159-8789-95a26cc1306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3337c271-7724-4606-8ae6-526470b6f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=min_max.fit_transform(df[['columnns']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d1fddf-612b-4cb8-ac5b-c37d10947471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5515562-27aa-44d9-a7f2-299c5c0dafa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values\n",
       "0  0.000000\n",
       "1  0.210526\n",
       "2  0.473684\n",
       "3  0.736842\n",
       "4  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df1,columns=['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e43ec-61a7-426d-8893-233897192b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
